{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing dataset and libraries, then reducing dataset using stratified sampling based on target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset/stroke_data.csv')\n",
    "\n",
    "no_stroke_group = df[df['stroke'] == 0]\n",
    "stroke_group = df[df['stroke'] == 1]\n",
    "\n",
    "group_size = 5000\n",
    "\n",
    "sampled_no_stroke_group = no_stroke_group.sample(n=group_size, random_state=42)\n",
    "sampled_stroke_group = stroke_group.sample(n=group_size, random_state=42)\n",
    "\n",
    "df = pd.concat([sampled_no_stroke_group,sampled_stroke_group])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEX\n",
      "min value:0.0\n",
      "max value: 1.0\n",
      "max common value: 1.0\n",
      "\n",
      "AGE\n",
      "min value:-5.0\n",
      "max value: 101.0\n",
      "max common value: 60.0\n",
      "\n",
      "HYPERTENSION\n",
      "min value:0\n",
      "max value: 1\n",
      "max common value: 0\n",
      "\n",
      "HEART_DISEASE\n",
      "min value:0\n",
      "max value: 1\n",
      "max common value: 0\n",
      "\n",
      "EVER_MARRIED\n",
      "min value:0\n",
      "max value: 1\n",
      "max common value: 1\n",
      "\n",
      "WORK_TYPE\n",
      "min value:0\n",
      "max value: 4\n",
      "max common value: 4\n",
      "\n",
      "RESIDENCE_TYPE\n",
      "min value:0\n",
      "max value: 1\n",
      "max common value: 1\n",
      "\n",
      "AVG_GLUCOSE_LEVEL\n",
      "min value:55.12\n",
      "max value: 271.74\n",
      "max common value: 94.39\n",
      "\n",
      "BMI\n",
      "min value:14.1\n",
      "max value: 78.0\n",
      "max common value: 31.4\n",
      "\n",
      "SMOKING_STATUS\n",
      "min value:0\n",
      "max value: 1\n",
      "max common value: 0\n",
      "\n",
      "STROKE\n",
      "min value:0\n",
      "max value: 1\n",
      "max common value: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    min_value = df[column].min()\n",
    "    max_value = df[column].max()\n",
    "    most_common_value = df[column].mode()[0]\n",
    "    \n",
    "    print(column.upper())\n",
    "    print(\"min value:\" + str(min_value))\n",
    "    print(\"max value: \" + str(max_value))\n",
    "    print(\"max common value: \" + str(most_common_value) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age feature contains values less than 0.\n",
    "Proceed to drop them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA ON ORIGINAL DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical features frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "categorical_features = ['sex', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\n",
    "\n",
    "def plot_categorical_bar_distribution():\n",
    "    categorical_df = df[categorical_features]\n",
    "    for column in categorical_features:\n",
    "        sns.catplot(x=column, kind=\"count\", data=categorical_df, height=6, aspect=1)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    stroke_counts = df['stroke'].value_counts()\n",
    "    labels = stroke_counts.index\n",
    "    sizes = stroke_counts.values\n",
    "    plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "    plt.axis('equal') \n",
    "    plt.show()\n",
    "        \n",
    "                \n",
    "plot_categorical_bar_distribution()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical features distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "numerical_features = ['age', 'avg_glucose_level', 'bmi']\n",
    "\n",
    "def plot_numerical_features():\n",
    "    numerical_df = df[numerical_features]\n",
    "    fig, axs = plt.subplots(nrows=len(numerical_features), ncols=2, figsize=(15, 6*len(numerical_features)))\n",
    "    \n",
    "    for i, column in enumerate(numerical_features):\n",
    "        sns.histplot(data=numerical_df, x=column, kde=True, ax=axs[i][0])\n",
    "\n",
    "        sns.boxplot(data=numerical_df, x=column, ax=axs[i][1])\n",
    "        \n",
    "        axs[i][0].set_title(f'{column} Distribution')\n",
    "        axs[i][1].set_title(f'{column} Boxplot')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_numerical_features()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc = {'figure.figsize':(11,9)})\n",
    "sns.heatmap(df.corr(),vmin=-1, vmax=1, annot = True, fmt='.1g',cmap= 'coolwarm')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "df_corr_with_stroke = df.corrwith(df['stroke']).drop('stroke')\n",
    "df_corr_with_stroke.plot(kind='bar', grid=True, title=\"Correlation with Stroke\", color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation and distribution plots between dataset numerical features and target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_plots():\n",
    "    numerical_features = ['age', 'avg_glucose_level', 'bmi']\n",
    "    for column in numerical_features:\n",
    "        print_box_plot(column)\n",
    "        print_strip_plot(column)\n",
    "        print_line_plot(column)\n",
    "        print_violin_plot(column)\n",
    "        \n",
    "    \n",
    "def print_strip_plot(column):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.stripplot(data=df, x='stroke', y=column)\n",
    "    plt.title('swarm Plot of Stroke by ' + column)\n",
    "    plt.xlabel('1: Stroke, 0: No Stroke')\n",
    "    plt.ylabel(column)\n",
    "    plt.show()\n",
    "    \n",
    "def print_box_plot(column):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(data=df, x=\"stroke\", y=column)\n",
    "    plt.title(f'Boxplot of {column} by Stroke')\n",
    "    plt.xlabel('1: Stroke, 0: No Stroke')\n",
    "    plt.ylabel(column)\n",
    "    plt.show()\n",
    "    \n",
    "def print_line_plot(column):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.lineplot(data=df, x='stroke', y=column)\n",
    "    plt.title(f'Lineplot of {column} by Stroke')\n",
    "    plt.xlabel('1: Stroke, 0: No Stroke')\n",
    "    plt.ylabel(column)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def print_violin_plot(column):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.violinplot(data=df, x=\"stroke\", y=column)\n",
    "    plt.title(f'Violinplot of {column} by Stroke')\n",
    "    plt.xlabel('1: Stroke, 0: No Stroke')\n",
    "    plt.ylabel(column)\n",
    "    plt.show()\n",
    "    \n",
    "print_plots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comparison between categorical features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "categorical_features = ['sex', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\n",
    "\n",
    "def print_stacked_barchart(column):\n",
    "    grouped = df.groupby([column, 'stroke']).size().unstack()\n",
    "    grouped.plot(kind='bar', stacked=True)\n",
    "    plt.title(f'Stroke Distribution by {column} ')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(title='Stroke', labels=['No Stroke', 'Stroke'])\n",
    "    plt.show()\n",
    "    \n",
    "def print_barchart(column):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.countplot(x='stroke', hue=column, data=df)\n",
    "    plt.title(f'Stroke Distribution by {column}')\n",
    "    plt.xlabel('Stroke')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(title=column)\n",
    "    plt.show()\n",
    "    \n",
    "def print_piechart(column):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.pieplot(x=column, hue='stroke', data=df)\n",
    "    plt.title(f'Stroke Distribution by {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(title='Stroke', labels=['No Stroke', 'Stroke'])\n",
    "    plt.show()\n",
    "    \n",
    "def print_plots():\n",
    "    for column in categorical_features:\n",
    "        print_stacked_barchart(column)\n",
    "        print_barchart(column)\n",
    "        ##print_piechart(column)\n",
    "\n",
    "print_plots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop('stroke', axis=1)\n",
    "y = df['stroke']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# Creazione del classificatore dell'albero decisionale\n",
    "tree_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Definizione della griglia degli iperparametri\n",
    "param_dist = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'min_samples_split': np.arange(2, 51, 2),\n",
    "    'min_samples_leaf': np.arange(1, 9),\n",
    "    'max_depth': [None, 5, 10, 15, 20]\n",
    "}\n",
    "\n",
    "# Utilizzo di RandomizedSearchCV per una ricerca efficiente degli iperparametri\n",
    "random_search = RandomizedSearchCV(tree_classifier, param_distributions=param_dist, n_iter=100, cv=10, scoring='roc_auc', random_state=42)\n",
    "\n",
    "# Misurazione del tempo di inizio per la ricerca degli iperparametri\n",
    "start_time_hyperparameter_search = time()\n",
    "random_search.fit(X_train, y_train)\n",
    "end_time_hyperparameter_search = time()\n",
    "hyperparameter_search_time = end_time_hyperparameter_search - start_time_hyperparameter_search\n",
    "\n",
    "# Ottenimento dei migliori parametri e del miglior stimatore\n",
    "best_params = random_search.best_params_\n",
    "best_tree_classifier = random_search.best_estimator_\n",
    "\n",
    "# Stampa dei migliori parametri e del loro ROC AUC score sul set di addestramento\n",
    "y_train_pred_prob = best_tree_classifier.predict_proba(X_train)[:, 1]\n",
    "roc_auc_train = roc_auc_score(y_train, y_train_pred_prob)\n",
    "\n",
    "# Addestramento del modello sull'intero set di addestramento e misurazione del tempo di addestramento\n",
    "start_time_training = time()\n",
    "best_tree_classifier.fit(X_train, y_train)\n",
    "end_time_training = time()\n",
    "dt_training_time = end_time_training - start_time_training\n",
    "\n",
    "# Previsioni sui set di addestramento e test\n",
    "y_train_pred = best_tree_classifier.predict(X_train)\n",
    "y_test_pred = best_tree_classifier.predict(X_test)\n",
    "\n",
    "# Stampa delle prestazioni sul set di addestramento\n",
    "print(\"\\nPrestazioni sul Set di Addestramento:\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "# Stampa delle prestazioni sul set di test\n",
    "print(\"\\nPrestazioni sul Set di Test:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Stampa dei migliori parametri e del tempo impiegato per la ricerca degli iperparametri e l'addestramento\n",
    "print(\"\\nMigliori Parametri:\", best_params)\n",
    "print(\"Tempo impiegato per la Ricerca degli Iperparametri:\", hyperparameter_search_time, \"secondi\")\n",
    "print(\"Tempo impiegato per l'Addestramento:\", dt_training_time, \"secondi\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "y_pred_prob = random_search.predict_proba(X_test)[:, 1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "print(\"AUC Score:\", roc_auc)\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test,y_test_pred)\n",
    "labels = [1,0]\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot();\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9542857142857143,\n",
       " 0.9514285714285714,\n",
       " 0.9614285714285714,\n",
       " 0.9371428571428572,\n",
       " 0.9571428571428572,\n",
       " 0.9585714285714285,\n",
       " 0.97,\n",
       " 0.9514285714285714,\n",
       " 0.9342857142857143,\n",
       " 0.9642857142857143]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "n_fold = 10\n",
    "folds = KFold(n_splits=n_fold, shuffle=True)\n",
    "\n",
    "accuracy_k_fold_dt = []\n",
    "\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X_train, y_train)):\n",
    "    X_train_fold, X_valid_fold = X_train.iloc[train_idx], X_train.iloc[valid_idx]\n",
    "    y_train_fold, y_valid_fold = y_train.iloc[train_idx], y_train.iloc[valid_idx]\n",
    "\n",
    "    random_search.fit(X_train_fold, y_train_fold)\n",
    "    # Fai previsioni sul set di validazione e calcola l'accuratezza\n",
    "    y_valid_pred = random_search.predict(X_valid_fold)\n",
    "    accuracy_k_fold_dt.append(accuracy_score(y_valid_fold, y_valid_pred))\n",
    "    \n",
    "\n",
    "accuracy_k_fold_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decision tree visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming random_search.best_estimator_ is your DecisionTreeClassifier and X_train is your training data\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "tree_plot = plot_tree(random_search.best_estimator_, filled=True, feature_names=X_train.columns, class_names=['0', '1'], rounded=True, impurity=False, fontsize=8)\n",
    "\n",
    "# Get the labels of each node and display them\n",
    "text = tree_plot[0]\n",
    "print(\"Node Labels:\\n\", text)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split, GridSearchCV\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscikit_learn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasClassifier\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Assuming `df` contains your DataFrame\n",
    "X = df.drop('stroke', axis=1)\n",
    "y = df['stroke']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define a function to create the model\n",
    "def create_model(optimizer='adam', units=16):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=units, activation='relu', input_shape=(10,)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# Wrap the Keras model in a scikit-learn compatible classifier\n",
    "keras_model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'optimizer': ['adam', 'sgd', 'rmsprop'],\n",
    "    'units': [8, 16, 32]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid = GridSearchCV(estimator=keras_model, param_grid=param_grid, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best accuracy\n",
    "print(\"Best parameters found: \", grid_result.best_params_)\n",
    "print(\"Best accuracy found: \", grid_result.best_score_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = grid_result.best_estimator_\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(\"Test accuracy of the best model: \", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
